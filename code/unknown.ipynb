{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38364bitbasecondafe405b4e042f431f82d437185aa356ab",
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer as Detok\n",
    "from textblob.wordnet import VERB\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# \n",
    "# BW: This script can be used to extract (possible) uknown concepts from a transcript with the use of the Textblob package\n",
    "#\n",
    "\n",
    "detokenizer= Detok()\n",
    "speaker_words = {}\n",
    "speaker_pattern = re.compile(r'^(\\w+?):(.*)$')\n",
    "\n",
    "ontologyList = ['po','approval', 'matching' , 'voucher' , 'VM', 'AP', 'PO' , 'address' ,'book', 'ledger', 'procurement' , 'currency', 'file-upload' ,'invoice', 'OCR', 'ocr', 'jd', 'JD', 'payment', 'qa' , 'QA' , 'workflow'] \n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.extend(['Yeah','yeah', 'Um','Well','Right', 'I', 'Yes','Uh','uh','Okay','okay', 'Sorry','sorry','Yep','yep','Uhh','uhh','Gonna', 'gonna','Gon na','gon na','Mm','mm','Wanna','Wan na','wanna','wan na','Ok','ok','OK','Oh','oh','Mhm','mhm','Sure','sure','Mhmm','mhmm','Jeez','jeez','Hi','hi','wow','Wow','True','true'])\n",
    "\n",
    "with open(\"transcript-splitted/.txt\", \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        current_speaker = None\n",
    "        for line in lines:\n",
    "                line = line.strip()\n",
    "                match = speaker_pattern.match(line)\n",
    "                if match is not None:\n",
    "                        current_speaker = match.group(1)\n",
    "                        line = match.group(2).strip()\n",
    "                        if current_speaker not in speaker_words.keys():\n",
    "                                speaker_words[current_speaker] = []\n",
    "                if current_speaker:\n",
    "                        words = [word.strip() for word in line.split(' ') if len(word.strip()) > 0]\n",
    "                        speaker_words[current_speaker].extend(words)\n",
    "\n",
    "dictget = lambda d, *k: [d[i] for i in k]\n",
    "#Add speakers here based on the total amount of speakers found in text\n",
    "spk_0 = dictget(speaker_words, 'spk_0')\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "#Filter stopwords from text and filter ontology words from text\n",
    "#Copy lines 43-64 for each speaker and change variable names accordingly\n",
    "text_0 = detokenizer.detokenize(spk_0)\n",
    "spk_0_tokenized = tokenizer.tokenize(text_0)\n",
    "filtered_words_0 = [word for word in spk_0_tokenized if word not in stopwords]\n",
    "new_text_0 = detokenizer.detokenize(filtered_words_0)\n",
    "for word in ontologyList:\n",
    "    new_text_0 = new_text_0.replace(word, \"\")\n",
    "blob_0 = TextBlob(new_text_0)\n",
    "#Get noun phrases from remaining text \n",
    "noun_list_0 = []\n",
    "for nouns in blob_0.noun_phrases:\n",
    "    noun_list_0.append(nouns)\n",
    "\n",
    "def removeElements(lst, k): \n",
    "    counted_0 = Counter(lst) \n",
    "    return [el for el in lst if counted_0[el] >= k]\n",
    "#Remove concepts that have a frequency that is less than the second parameter\n",
    "removed_0=removeElements(noun_list_0, 1)\n",
    "countedFinished_0 = Counter(removed_0)\n",
    "#Put resulting concepts in Dataframe\n",
    "df_0 = pd.DataFrame.from_dict(countedFinished_0, orient='index').reset_index()\n",
    "df_0.columns = ['Noun Phrase', 'Frequency']\n",
    "df_0.insert(2, \"Speaker\", \"spk_0\", True) \n",
    "df_new_0 = df_0.sort_values(by=['Frequency'], ascending=False)\n",
    "#Combine dataframes based on the amount of speakers\n",
    "frames = [df_new_0]\n",
    "result = pd.concat(frames)\n",
    "result_sorted = result.sort_values(by=['Frequency'], ascending=False)\n",
    "result_sorted.to_excel(\"output/.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}